{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZIzfN7d2mYt",
        "colab_type": "text"
      },
      "source": [
        "# TC and REG Data Cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# authenticate \n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Connect to google sheets\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cleaning Reg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfEcSre328Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# Date and Time\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "# Options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 20)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "# Import data\n",
        "path = \"/content/drive/My Drive/\"\n",
        "df_reg_raw = pd.read_csv(path + \"Finder Minder/Data/new/JobReady.registrations.csv\")\n",
        "\n",
        "# replace with Google Sheet needed, already Variable summary\n",
        "google_sheet_url = 'https://docs.google.com/spreadsheets/d/1v1hhxC23KJ7t-S26MAwBpCqo1JR2mdrvzuzMWt7_hPU/edit#gid=897172359'\n",
        "wb = gc.open_by_url(google_sheet_url)\n",
        "\n",
        "# get Google Sheet worksheet\n",
        "sheet1 = wb.worksheet('JobReady Reg')\n",
        "data = sheet1.get_all_values()\n",
        "regVarSummary = pd.DataFrame(data[1:], columns=data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydix6azKM224",
        "colab_type": "code",
        "outputId": "5aff01e5-70fc-487f-caed-c8319fda39a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# helper functions\n",
        "\n",
        "def StringToDate(df, cols, date_format='%Y-%m-%d %H:%M:%S', to_float=False):\n",
        "  new_df = pd.DataFrame(columns = cols)\n",
        "  for col in cols:\n",
        "    try:\n",
        "      new_df[col] = pd.to_datetime(df[col], format = date_format, errors='coerce')\n",
        "      if to_float == True:\n",
        "        new_df[col] = new_df[col].apply(lambda date: (date - datetime.datetime(1970, 1, 1)) / datetime.timedelta(days=1))#.astype('float')\n",
        "    except:\n",
        "      print(col)\n",
        "  return new_df\n",
        "  \n",
        "\n",
        "def ColumnsWithTreatmentReg(treatment):\n",
        "    return list(regVarSummary['ColName'][regVarSummary['Treatment'] == treatment])\n",
        "\n",
        "\n",
        "# given df and columns to check, get missing values percentage into a dictionary\n",
        "def MissingCounts(df, cols): \n",
        "    missingPercent = {}\n",
        "    for col in cols:\n",
        "        missingPercent[col] = round(df[col].isnull().sum() / len(df), 2)\n",
        "    return missingPercent\n",
        "\n",
        "# prints value counts and percent missing in sorted missing\n",
        "def PrintStats(df, missingDict):\n",
        "\n",
        "    for key, value in sorted(missingDict.items(), key=lambda item: item[1]):\n",
        "        print(key)\n",
        "        print(\"-\"*len(key))\n",
        "        \n",
        "        print(df[key].value_counts(dropna = False))\n",
        "        print(df[key].value_counts(dropna = False, normalize = True) * 100)\n",
        "        print('missing %f \\n' % value)\n",
        "\n",
        "# creates columns for date\n",
        "def SeparateDate(df, columns):\n",
        "    for col in columns:\n",
        "        df[col + 'year'] = pd.DatetimeIndex(df[col]).year\n",
        "        df[col + 'month'] = pd.DatetimeIndex(df[col]).month\n",
        "        df[col + 'day'] = pd.DatetimeIndex(df[col]).day\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#################\n",
        "# Summary\n",
        "#################\n",
        "\n",
        "# label encoder can turn values in columns into column_value columns\n",
        "# put all cols to drop in this array\n",
        "dropCols = []\n",
        "\n",
        "#################\n",
        "# Drop cols\n",
        "#################\n",
        "\n",
        "df_reg = df_reg_raw.dropna(1,thresh = int(len(df_reg_raw)*0.01)) # 99% missing\n",
        "dropVariableSummary =  ColumnsWithTreatment('DROP')\n",
        "dropIdenticalValues = [col for col in df_reg.columns if (df_reg[col].nunique() == 1) & (df_reg[col].isnull().sum() == 0)]\n",
        "dropOther = [\"LoadDateTime\",              # SQL query data \n",
        "             \"notinworkforcefor3years\"]   # Indicator w/out info (all 0 or missing)\n",
        "df_reg.drop(dropVariableSummary + dropIdenticalValues + dropOther, axis=1, inplace=True)\n",
        "\n",
        "# Drop rows without target variable, still ongoing apprenticeships\n",
        "df_reg = df_reg[df_reg['actualenddate'].isnull().eq(False)]\n",
        "\n",
        "# get target variable and econde\n",
        "df_reg['GRADUATED'] = np.where(df_reg['endreason'] == 'CMPS', 1, 0)\n",
        "\n",
        "#################\n",
        "# Read datetimes\n",
        "#################\n",
        "\n",
        "# drop greater than 90 missing\n",
        "\n",
        "dateCols = ColumnsWithTreatment('DATE') # get cols with date treatment\n",
        "df_reg[dateCols] = StringToDate(df_reg, dateCols,'%Y-%m-%d') # convert them all to datetime format\n",
        "\n",
        "# replace values of expectedenddate with expected enddate overrride (updated expected enddate), ignore \n",
        "df_reg['expectedenddate'] = np.where(df_reg['expectedenddate_override'].isnull().eq(False), df_reg['expectedenddate_override'], df_reg['expectedenddate'])\n",
        "\n",
        "# get number of years to complete\n",
        "df_reg['YEARS_TO_COMPLETE'] = (df_reg['actualenddate'] - df_reg['startdate']).astype('timedelta64[D]')/365\n",
        "\n",
        "# removes bad years but also some 0's are actually 1s so need a better way to find years to complete\n",
        "df_reg = df_reg[df_reg['YEARS_TO_COMPLETE'] >= 0].reset_index(drop=True)\n",
        "\n",
        "# actualenddate - 0.0000\n",
        "# commencementvisitdate - 0.230000, commencementvisitdate around the same as start date, but not duplicate\n",
        "# recommencementdate - 0.830000  \n",
        "# sixmonthspointdate - 0.300000, on average it is actually 6 months\n",
        "# twelvemonthpointdate - 0.670000 on average it is actually 12 months\n",
        "# twentyfourmonthpointdate - 0.840000 on average it is actually 24 months\n",
        "# initialservicingdateofcontact - 0.420000 \n",
        "# intermediateservicingdateofcontact - 0.71000 \n",
        "# workplacefromdate - 0.320000 Experience at employer\n",
        "# sta_approved_date - 0.420000 \n",
        "# sta_registered_date - 0.430000 Check if these events occur before completion/dropout, on average it seems that sta_registered_date is before actualenddate\n",
        "# 2207-11-03 should be 2007-11-03, change 1900-01-01 to null\n",
        "\n",
        "df_reg['sta_registered_date'] = df_reg['sta_registered_date'].mask(df_reg['sta_registered_date'].dt.year == 1900, \n",
        "                                                                   np.datetime64('NaT'))\n",
        "\n",
        "# daterecievedfromsta - 0.510000 Check if these events occur before completion/dropout, on average it seems that daterecievedfromsta is after actualenddate\n",
        "# sta_notified_end_date - 0.660000 on average it seems that sta_notified_end_date is about same as expectedenddate, 2107-01-13 should be 2017-01-13\n",
        "df_reg['daterecievedfromsta'] = df_reg['daterecievedfromsta'].mask(df_reg['daterecievedfromsta'].dt.year == 2107, \n",
        "                                                                   df_reg['daterecievedfromsta'] + pd.offsets.DateOffset(year=2017))\n",
        "\n",
        "df_reg['daterecievedfromsta'] = df_reg['daterecievedfromsta'].mask(df_reg['daterecievedfromsta'].dt.year == 2208, \n",
        "                                                                   df_reg['daterecievedfromsta'] + pd.offsets.DateOffset(year=2008))\n",
        "df_reg[dateCols] = StringToDate(df_reg, dateCols,'%Y-%m-%d')\n",
        "dropCols += ['expectedenddate_override']\n",
        "\n",
        "#################\n",
        "# Binary\n",
        "#################\n",
        "\n",
        "# checking binary\n",
        "binaryCols = ColumnsWithTreatment('BINARY')\n",
        "\n",
        "# filling in all the missing values to -99\n",
        "# for col in binaryCols:\n",
        "#     df[col] = df[col].fillna(value = -99)\n",
        "\n",
        "# drop by default those over 90%\n",
        "dropCols += ['isthisaustralianapprenticeshipsupportedbytheschool', 'assessedasawtuineligible', \n",
        "             'ineligible_supportforadultaustralianapprentices', 'saaa_tfn_supplied', 'redundant', \n",
        "             'intensivesupportassistance', 'incomesupport', 'twentyfourmonthmodeofcontact']\n",
        "\n",
        "#################\n",
        "# Small Cat \n",
        "#################\n",
        "\n",
        "# Strip and lower all string fields\n",
        "df_obj = df_reg.select_dtypes(['object']).apply(lambda x: x.str.lower())\n",
        "df_reg[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
        "\n",
        "# Check small category cleaning\n",
        "colsSmallCat = ColumnsWithTreatment('SMALL_CAT')\n",
        "missingSCat = MissingCounts(df_reg, colsSmallCat)\n",
        "\n",
        "# standardising\n",
        "# getting rid of prefix\n",
        "df_reg['aqflevel'] = df_reg['aqflevel'].str.strip('aqf')\n",
        "\n",
        "# fixing formatting for sta\n",
        "df_reg['sta'] = df_reg['sta'].str.replace(\"-sta\", \"\")\n",
        "df_reg['sta'] = df_reg['sta'].str.strip()\n",
        "\n",
        "# shortening stacommunicationstatusmessage for encoding\n",
        "df_reg['stacommunicationstatusmessage'] = df_reg['stacommunicationstatusmessage'].map({'details amended or entered by sta': 3,\n",
        "                                                                                       'sta confirmation recieved': 2,\n",
        "                                                                                       'details amended by sta during confirmation': 4,\n",
        "                                                                                       'sta confirmation pending': 1}) \n",
        "\n",
        "# filling in nas with -99\n",
        "# for col in colsSmallCat:\n",
        "#     df_reg[col] = df_reg[col].fillna(value = -99)\n",
        "\n",
        "# dropping over 90% missing\n",
        "dropCols += ['saaa_recip', 'assessedasctlsineligiblecomments', 'assessedasawtuineligiblecomments']\n",
        "\n",
        "\n",
        "#################\n",
        "# Nums\n",
        "#################\n",
        "\n",
        "# Check large category cleaning\n",
        "nums = ColumnsWithTreatment('NUMERICAL')\n",
        "\n",
        "# nominaldurationinmonths 0.32 nominal duration for what?\n",
        "\n",
        "# creating categories for hoursperweek 0.87, according to maximum hours for apprenticeship Google search max per week is 38 hours\n",
        "df_reg['overmaxhoursperweek'] = np.where(df_reg['hoursperweek'] > 38, 1, 0)\n",
        "# high hours\n",
        "df_reg['highhoursperweek'] = np.where((df_reg['hoursperweek'] <= 38) & (df_reg['hoursperweek'] > 16), 1, 0)\n",
        "# two days is 16 hours\n",
        "df_reg['mediumhoursperweek'] = np.where((df_reg['hoursperweek'] <= 16) & (df_reg['hoursperweek'] > 8), 1, 0)\n",
        "# normal day is 8 hours, 9-5\n",
        "df_reg['lowhoursperweek'] = np.where(df_reg['hoursperweek'] <= 8, 1, 0)\n",
        "\n",
        "# numberofemployeesinworkplace 0.32 - group into large, medium small companies\n",
        "# larger than 75th quantile is big\n",
        "# 25th to 75th quantile is medium\n",
        "# under 25th quantile small\n",
        "df_reg['at_small_company'] = np.where(df_reg['hoursperweek'] <= 10, 1, 0)\n",
        "df_reg['at_medium_company'] = np.where((df_reg['hoursperweek'] <= 450) & (df_reg['hoursperweek'] > 10), 1, 0)\n",
        "df_reg['at_big_company'] = np.where(df_reg['hoursperweek'] > 450, 1, 0)\n",
        "\n",
        "# ftequivalentmonths  0.87 full time equivalent months\n",
        "df_reg['ft_small'] = np.where(df_reg['ftequivalentmonths'] <= 9, 1, 0)\n",
        "df_reg['ft_medium'] = np.where((df_reg['ftequivalentmonths'] <= 61) & (df_reg['hoursperweek'] > 9), 1, 0)\n",
        "df_reg['ft_big'] = np.where(df_reg['ftequivalentmonths'] > 61, 1, 0)\n",
        "\n",
        "# filling in nas with -99\n",
        "#for col in colsLargeCat:\n",
        "#    df_reg[col] = df_reg[col].fillna(value = -99)\n",
        "\n",
        "# no variables over 90% missing\n",
        "\n",
        "#################\n",
        "# Large Cat \n",
        "#################\n",
        "\n",
        "# Check large category cleaning\n",
        "colsLargeCat = ColumnsWithTreatment('LARGE_CAT')\n",
        "\n",
        "# employer_id 0.0 seems to have different lengths, but thats ok, unique identifier and could be used for econ data merge\n",
        "# endreason 0.0 keep as it is important\n",
        "\n",
        "# split qualification code into prefix and suffix\n",
        "df_reg['qc_prefix'] = df_reg['qualificationcode'].map(lambda x: x[:2])\n",
        "df_reg['qc_suffix'] = df_reg['qualificationcode'].map(lambda x: x[2:])\n",
        "\n",
        "# dropping tyims_created_by, tyims_updated_by as they are created by the system, and are system operations, not necessary\n",
        "dropCols += ['tyims_created_by', 'tyims_updated_by']\n",
        "\n",
        "# dropping workplaceaddline1 as there are many variables there that are uncategorisable and location information is already in workplacepostcode\n",
        "dropCols += ['workplaceaddline1']\n",
        "# try to standardise and group workplaceemployername\n",
        "# map all employer ids to one workplaceemployername, deduplicate\n",
        "match = {}\n",
        "for i in df_reg['employer_id'].unique():\n",
        "    for idx, val in df_reg[df_reg['employer_id'] == i]['workplaceemployername'].value_counts().iteritems():\n",
        "        match[i] = idx\n",
        "        break\n",
        "\n",
        "df_reg['workplaceemployername'] = df_reg['employer_id'].map(match).fillna(df_reg['workplaceemployername'])\n",
        "\n",
        "# workplacepostcode 0.0 information about location, can replace workplacesuburb and is good enough for workplaceaddline1\n",
        "# dropping workplacesuburb as this is basically same as workplacepostcode\n",
        "dropCols += ['workplacesuburb']\n",
        "\n",
        "# staid - 0.01 split to prefix and suffix, e.g. 2269481\\3\n",
        "df_reg['staid_prefix'], df_reg['staid_suffix'] = df_reg['staid'].str.split('\\\\',2).str\n",
        "\n",
        "# rto_id 0.01 seems to have different lengths for id, not sure why RTO Registered Training Organisation\n",
        "\n",
        "# barcode 0.02 not sure what for, aac12/98268, 372489\n",
        "df_reg['barcode_prefix'] = df_reg['barcode'].astype(str).str[:5]\n",
        "df_reg['barcode_suffix'] = df_reg['barcode'].astype(str).str[6:]\n",
        "\n",
        "# deewrfilenumber 0.02 Department of Education, Employment and Workplace Relations, aac10/319930, 372489\n",
        "df_reg['deewrfilenumber_prefix'] = df_reg['deewrfilenumber'].astype(str).str[:5]\n",
        "df_reg['deewrfilenumber_suffix'] = df_reg['deewrfilenumber'].astype(str).str[6:]\n",
        "\n",
        "# commencementvisitfieldofficer 0.24 diago rodrigues, for each commencement visit field officer, create id, create exist binary\n",
        "df_reg['ITC_experience'] = np.where(df_reg['commencementvisitfieldofficer'].isnull(), 0, 1)\n",
        "\n",
        "\n",
        "# employername 0.32 repeated and weaker version of workplaceemployername, more missing, same info, drop\n",
        "dropCols += ['employername']\n",
        "\n",
        "# tyims_contractid 0.32, keep as it could be useful for merging trc, mapping contract details\n",
        "\n",
        "# anzscooccupationcode 0.32, keep and do nothing, might be useful no formatting needed\n",
        "\n",
        "# site_code 0.35, 64, keep might be useful\n",
        "\n",
        "# apprenticeshipqualificationid 0.36, 3525 might be useful keep\n",
        "\n",
        "# registration_manager_id 0.36, 51, might be useful keep as an id\n",
        "\n",
        "# employerbankaccountidlinkedtoreg 0.61, 41780, employerhasbankaccount binary relates\n",
        "# but double missing so drop, no need for bank accounts details just employer details that don't describe the employer, drop\n",
        "dropCols += ['employerbankaccountidlinkedtoreg']\n",
        "\n",
        "# clientcontactphone 0.79, 73137, a lot of unique values, no relevance to apprenticeship, just employer details that don't describe the employer\n",
        "# area code can be found with postcode\n",
        "dropCols += ['clientcontactphone']\n",
        "\n",
        "# sta_notified_end_date_updated_by 0.87, 30, weird values but keep\n",
        "\n",
        "# certificatenumber 0.88, 45190, many unique numbers, as well as missing values and this probably happens after Traineeship/apprenticeship\n",
        "# dropCols += ['certificatenumber']\n",
        "# feature engineer says keep for some reason so i guess ill keep\n",
        "\n",
        "# recommencementvisitfieldofficer 0.93, 1420, exception will keep, somehow separate recommenced apprentices\n",
        "\n",
        "# filling in nas with -99\n",
        "# for col in colsLargeCat:\n",
        "#     df_reg[col] = df_reg[col].fillna(value = -99)\n",
        "\n",
        "# dropping more than 90% missing - ineligible_employer_reason_type_id 0.95\n",
        "dropCols += ['ineligible_employer_reason_type_id']\n",
        "\n",
        "# drop columns and merge binary to the df_reg\n",
        "df_reg.drop(columns = dropCols, axis=1, inplace=True)\n",
        "df_reg.drop(columns = ['employee_id'], axis=1, inplace=True)\n",
        "\n",
        "# export the final dataset\n",
        "df_reg.to_csv('registrations_cleaned_v2.csv', index=False)\n",
        "\n",
        "!cp registrations_cleaned_v2.csv \"/content/drive/My Drive/Finder Minder/Data/new/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cleaning TRC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import variable summary\n",
        "file_url = \"https://docs.google.com/spreadsheets/d/1v1hhxC23KJ7t-S26MAwBpCqo1JR2mdrvzuzMWt7_hPU/edit#gid=1634553018\"\n",
        "wb = gc.open_by_url(file_url)\n",
        "sheet_name = 'JobReady TCs'\n",
        "variableSummary = pd.DataFrame(wb.worksheet(sheet_name).get_all_values()[1:],\n",
        "                               columns = wb.worksheet(sheet_name).get_all_values()[0])\n",
        "\n",
        "# Import data\n",
        "path = \"/content/drive/My Drive/Finder Minder/\"\n",
        "df_trc_raw = pd.read_csv(path + \"Data/new/JobReady.training_contracts.csv\")\n",
        "df_trc = pd.read_csv(\"/content/drive/My Drive/Finder Minder/Data/new/training_contract_partial_clean.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def StringToDate(df, cols, date_format='%Y-%m-%d %H:%M:%S', to_float=False):\n",
        "  new_df = pd.DataFrame(columns = cols)\n",
        "  for col in cols:\n",
        "    try:\n",
        "      new_df[col] = pd.to_datetime(df[col], format = date_format, errors='coerce')\n",
        "      if to_float == True:\n",
        "        new_df[col] = new_df[col].apply(lambda date: (date - datetime.datetime(1970, 1, 1)) / datetime.timedelta(days=1))#.astype('float')\n",
        "    except:\n",
        "      print(col)\n",
        "  return new_df\n",
        "\n",
        "def ColumnsWithTreatment(treatment):\n",
        "    return list(variableSummary['ColName'][variableSummary['Treatment'] == treatment])\n",
        "\n",
        "def PrintValueCounts(df, cols):\n",
        "    for col in cols:\n",
        "        print(col)\n",
        "        print(\"-\"*len(col))\n",
        "        print(df[col].value_counts())\n",
        "        print(\"missing\", df[col].isnull().mean())\n",
        "        print()\n",
        "\n",
        "def EncodeIfStringContains(df, col, substring):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        df = (dataframe)\n",
        "        col = (string) column name\n",
        "        substring = (list[string]) a list of the substrings you want to check if it contains\n",
        "    \"\"\"\n",
        "    for sub in substring:\n",
        "        df[col+\"_\"+substring] = np.where(df[col].str.contains(sub), 1, 0)\n",
        "print(df_trc.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#################\n",
        "# Drop cols\n",
        "#################\n",
        "\n",
        "df_trc = df_trc_raw.dropna(1,thresh = int(len(df_trc_raw)*0.01)) # 99% missing\n",
        "\n",
        "# Converting Missing column to float\n",
        "variableSummary['Missing'] = pd.to_numeric(variableSummary['Missing'])\n",
        "\n",
        "dropNullCols= list(variableSummary['ColName'][variableSummary['Missing'] >= 0.90])\n",
        "\n",
        "dropVariableSummary =  ColumnsWithTreatment('DROP')\n",
        "#print(dropVariableSummary)\n",
        "dropIdenticalValues = [col for col in df_trc.columns if (df_trc[col].nunique() == 1) & (df_trc[col].isnull().sum() == 0)]\n",
        "#dropOther = [\"LoadDateTime\", \"notinworkforcefor3years\"]   # Indicator w/out info (all 0 or missing)\n",
        "#print(dropIdenticalValues)\n",
        "df_trc.drop(dropVariableSummary + dropIdenticalValues, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# binary\n",
        "binaryCols = ColumnsWithTreatment('BINARY')\n",
        "for i in binaryCols:\n",
        "    if i in df_trc.columns:\n",
        "        df_trc[i].fillna(-99, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "date_cols = ColumnsWithTreatment('DATE')\n",
        "df_trc[date_cols] = StringToDate(df_trc, date_cols,'%Y-%m-%d')\n",
        "\n",
        "\n",
        "# Ensuring all the data types have been converted\n",
        "for i in date_cols:\n",
        "    if i in df_trc.columns:\n",
        "        print(df_trc[i].dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_cat_cols = ColumnsWithTreatment('SMALL_CAT')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trc.guardianpostalstate = df_trc_raw.guardianpostalstate\n",
        "\n",
        "state_dict = {'NSW': 0, 'QLD': 1, 'ACT': 2, 'VIC': 3, 'WA': 4, 'TAS': 5, 'SA': 5, 'NT': 6}\n",
        "\n",
        "for i in df_trc.employerpostalstate.astype('str').unique():\n",
        "    for j in state_dict.keys():\n",
        "        if re.search(j, i, re.IGNORECASE):\n",
        "            #df_trc.at[i,'employerpostalstate']= j\n",
        "            df_trc.loc[df_trc['employerpostalstate'] == i, 'employerpostalstate'] = j\n",
        "df_trc.employerpostalstate.fillna(-99)\n",
        "df_trc.loc[df_trc['employerpostalstate'] == \" \", 'employerpostalstate'] = -99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trc.typeofemploymentarrangement = df_trc_raw.typeofemploymentarrangement\n",
        "\n",
        "for idx, val in df_trc['typeofemploymentarrangement'].value_counts().iteritems():\n",
        "    if val < 4000:\n",
        "        df_trc['typeofemploymentarrangement'][df_trc['typeofemploymentarrangement'] == idx] = -99\n",
        "\n",
        "df_trc.typeofemploymentarrangement.fillna(-99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strip and lower all string fields\n",
        "df_obj = df_trc.select_dtypes(['object']).apply(lambda x: x.str.lower())\n",
        "df_trc[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
        "drop = []\n",
        "\n",
        "# Apprenticeship type\n",
        "# Encoding Traineeship as 0 and Apprenticeship as 1\n",
        "df_trc['typeofapprenticeship'] = df_trc['typeofapprenticeship'].map({'T': 0, 'A': 1, 'Traineeship': 0, 'Apprenticeship': 1, 'S': 2, 'Trainee Apprentice (NSW Only)': 3})\n",
        "# Replacing missing values with -99\n",
        "df_trc['typeofapprenticeship'].fillna(-99)\n",
        "\n",
        "# Employer type\n",
        "# df_trc['employertype'] = df_trc['employertype'].astype(str)\n",
        "# Filling null values with -99\n",
        "df_trc['employertype'].fillna(-99)\n",
        "\n",
        "# Attendence type: Full-Time is 1, Part-Time is 0, School is 2, missing values -99\n",
        "df_trc['attendancetype'] = df_trc['attendancetype'].map({'Full-Time': 1, 'Part-Time': 0, '1.0': 1, '2.0': 0, 'S': 2, 'School-Based': 2})\n",
        "df_trc['attendancetype'].fillna(-99)\n",
        "\n",
        "# Current year\n",
        "df_trc['attendingschoollevel_senior'] = np.where(df_trc['attendingschoollevel'].isin([11,12]), 1, 0)\n",
        "df_trc['attendingschoollevel_junior'] = np.where(df_trc['attendingschoollevel'] <= 10, 1, 0)\n",
        "drop += ['highestcompletedschoollevel']\n",
        "\n",
        "# Highest completed year\n",
        "df_trc['highestcompletedschoollevel_graduated'] = np.where(df_trc['highestcompletedschoollevel'] == 12, 1, 0)\n",
        "df_trc['highestcompletedschoollevel_senior'] = np.where(df_trc['highestcompletedschoollevel'].isin([10,11]), 1, 0)\n",
        "df_trc['highestcompletedschoollevel_junior'] = np.where(df_trc['highestcompletedschoollevel'] <= 9, 1, 0)\n",
        "df_trc['highestcompletedschoollevel_graduated'].fillna(-99)\n",
        "df_trc['highestcompletedschoollevel_senior'].fillna(-99)\n",
        "df_trc['highestcompletedschoollevel_junior'].fillna(-99)\n",
        "drop += ['highestcompletedschoollevel']\n",
        "\n",
        "# States\n",
        "\n",
        "state_dict = {'NSW': 0, 'QLD': 1, 'ACT': 2, 'VIC': 3, 'WA': 4, 'TAS': 5, 'SA': 5, 'NT': 6}\n",
        "df_trc['guardianpostalstate'] = df_trc['guardianpostalstate'].map(state_dict)\n",
        "df_trc['guardianpostalstate'].fillna(-99)\n",
        "\n",
        "for i in df_trc.employerpostalstate.astype('str').unique():\n",
        "    for j in state_dict.keys():\n",
        "        if re.search(j, i, re.IGNORECASE):\n",
        "            df_trc.loc[df_trc['employerpostalstate'] == i, 'employerpostalstate'] = j\n",
        "df_trc.employerpostalstate.fillna(-99)\n",
        "df_trc.loc[df_trc['employerpostalstate'] == \" \", 'employerpostalstate'] = -99\n",
        "\n",
        "for i in df_trc.appresidentialstate.astype('str').unique():\n",
        "    for j in state_dict.keys():\n",
        "        if re.search(j, i, re.IGNORECASE):\n",
        "            df_trc.loc[df_trc['appresidentialstate'] == i, 'appresidentialstate'] = j\n",
        "df_trc.appresidentialstate.fillna(-99)\n",
        "df_trc.loc[df_trc['appresidentialstate'] == \" \", 'appresidentialstate'] = -99\n",
        "\n",
        "for i in df_trc.workplacestate.astype('str').unique():\n",
        "    for j in state_dict.keys():\n",
        "        if re.search(j, i, re.IGNORECASE):\n",
        "            df_trc.loc[df_trc['workplacestate'] == i, 'workplacestate'] = j\n",
        "df_trc.workplacestate.fillna(-99)\n",
        "df_trc.loc[df_trc['workplacestate'] == \" \", 'workplacestate'] = -99\n",
        "\n",
        "# Indigenous\n",
        "df_trc['indigenousstatus'].fillna(-99)\n",
        "\n",
        "# Remove categories with small count\n",
        "# for val in df_trc['modeofdeliveryid'].unique():\n",
        "#     if df_trc['modeofdeliveryid'][df_trc['modeofdeliveryid'] == val].sum() < 10000:\n",
        "#         df_trc['modeofdeliveryid'][df_trc['modeofdeliveryid'] == val] = np.NaN\n",
        "df_trc.modeofdeliveryid.fillna(-99)\n",
        "\n",
        "# Sex and nationality\n",
        "df_trc['appsex'] = np.where(df_trc['appsex'].isin(['M','Male']), 0, 1)\n",
        "df_trc['appcitizenship'] = np.where(df_trc['appcitizenship'] == 'a', 0, 1)\n",
        "\n",
        "# employment arrangement\n",
        "for idx, val in df_trc['typeofemploymentarrangement'].value_counts().iteritems():\n",
        "    if val < 4000:\n",
        "        df_trc['typeofemploymentarrangement'][df_trc['typeofemploymentarrangement'] == idx] = -99\n",
        "#print(df_trc.typeofemploymentarrangement.value_counts())\n",
        "df_trc.typeofemploymentarrangement.fillna(-99)\n",
        "\n",
        "df_trc.employertype.fillna(-99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# numeric cols\n",
        "numericCols = ColumnsWithTreatment('NUMERICAL')\n",
        "for i in numericCols:\n",
        "    if i in df_trc.columns:\n",
        "        # print(i)\n",
        "        df_trc[i].fillna(-99, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# large cat\n",
        "largeCols = ColumnsWithTreatment('LARGE_CAT')\n",
        "for i in largeCols:\n",
        "    if i in df_trc.columns:\n",
        "        df_trc[i].fillna(-99, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trc['approvingsta'].value_counts()\n",
        "#df_trc['approvingsta'].apply(str).apply(lambda x: x.str.split('-')[0])\n",
        "\n",
        "df_trc['approvingsta'].apply(str)\n",
        "for idx, val in df_trc['approvingsta'].astype('str').iteritems():\n",
        "    if str(val).split('-'):\n",
        "        states = str(val).split('-')[0]\n",
        "        df_trc.loc[idx, 'approvingsta'] = states\n",
        "        #df_trc.loc[df_trc['approvingsta'] == i, 'employerpostalstate'] = j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trc['approvingsta'].value_counts()\n",
        "\n",
        "for i in df_trc.approvingsta.astype('str').unique():\n",
        "    for j in state_dict.keys():\n",
        "        if re.search(j, i, re.IGNORECASE):\n",
        "            df_trc.loc[df_trc['approvingsta'] == i, 'approvingsta'] = j\n",
        "df_trc.approvingsta.fillna(-99)\n",
        "df_trc['approvingsta'] = df_trc['approvingsta'].astype('str').replace({'1320-sta': -99, 'nan': -99})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EncodeIfStringContains(df_trc, 'qualificationlevelobtained', ['certificate', 'ii', 'iii', 'child', 'bachelor', 'engineering', 'construction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_trc.to_csv('TC_clean1.csv', index=False)\n",
        "!cp TC_clean1.csv \"drive/My Drive/Finder Minder/Data\""
      ]
    }
  ]
}